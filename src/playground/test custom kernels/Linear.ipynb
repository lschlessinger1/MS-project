{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "from GPy.kern.src.kern import Kern\n",
    "from GPy.core import Param\n",
    "from GPy.util.linalg import tdot\n",
    "from GPy.kern.src.psi_comp import PSICOMP_Linear\n",
    "\n",
    "from paramz.transformations import Logexp\n",
    "from paramz.caching import Cache_this\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented Implementation of Linear Kernel\n",
    "GPy's `Linear` kernel is simply the scaled dot product of its inputs. The scale shifted kernel is defined as:\n",
    "\n",
    "$k_{LIN}(x,x') = \\sigma_b^{2} + \\sigma_v^{2}\\left(x - \\ell\\right)\\left(x' - \\ell\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinScaleShift(Kern):\n",
    "    \"\"\"\n",
    "    Linear kernel\n",
    "\n",
    "    .. math::\n",
    "\n",
    "       k(x,y) = \\sum_{i=1}^{\\\\text{input_dim}} \\sigma^2_i x_iy_i\n",
    "\n",
    "    :param input_dim: the number of input dimensions\n",
    "    :type input_dim: int\n",
    "    :param variances: the vector of variances :math:`\\sigma^2_i`\n",
    "    :type variances: array or list of the appropriate size (or float if there\n",
    "                     is only one variance parameter)\n",
    "    :param ARD: Auto Relevance Determination. If False, the kernel has only one\n",
    "                variance parameter \\sigma^2, otherwise there is one variance\n",
    "                parameter per dimension.\n",
    "    :type ARD: Boolean\n",
    "    :rtype: kernel object\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, variances=None, shifts=None, ARD=False, active_dims=None, name='lin'):\n",
    "        super(LinScaleShift, self).__init__(input_dim, active_dims, name)\n",
    "        self.ARD = ARD\n",
    "        if not ARD:\n",
    "            if variances is not None:\n",
    "                variances = np.asarray(variances)\n",
    "                assert variances.size == 1, \"Only one variance needed for non-ARD kernel\"\n",
    "            else:\n",
    "                variances = np.ones(1)\n",
    "            if shifts is not None:\n",
    "                shifts = np.asarray(shifts)\n",
    "                assert shifts.size == 1, \"Only one shift needed for non-ARD kernel\"\n",
    "            else:\n",
    "                shifts = np.ones(1)\n",
    "        else:\n",
    "            if variances is not None:\n",
    "                variances = np.asarray(variances)\n",
    "                assert variances.size == self.input_dim, \"bad number of variances, need one ARD variance per input_dim\"\n",
    "            else:\n",
    "                variances = np.ones(self.input_dim)\n",
    "            if shifts is not None:\n",
    "                shifts = np.asarray(shifts)\n",
    "                assert shifts.size == self.input_dim, \"bad number of shifts, need one ARD shift per input_dim\"\n",
    "            else:\n",
    "                shifts = np.ones(self.input_dim)\n",
    "\n",
    "        self.variances = Param('variances', variances, Logexp())\n",
    "        self.shifts = Param('shifts', shifts, Logexp())\n",
    "        self.link_parameter(self.variances)\n",
    "        self.link_parameter(self.shifts)\n",
    "        self.psicomp = PSICOMP_Linear()\n",
    "\n",
    "    def to_dict(self):\n",
    "        input_dict = super(LinScaleShift, self)._save_to_input_dict()\n",
    "        input_dict[\"class\"] = \"GPy.kern.LinScaleShift\"\n",
    "        input_dict[\"variances\"] = self.variances.values.tolist()\n",
    "        input_dict[\"shifts\"] = self.shifts.values.tolist()\n",
    "        input_dict[\"ARD\"] = self.ARD\n",
    "        return input_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_from_input_dict(kernel_class, input_dict):\n",
    "        useGPU = input_dict.pop('useGPU', None)\n",
    "        return LinScaleShift(**input_dict)\n",
    "\n",
    "    @Cache_this(limit=3)\n",
    "    def K(self, X, X2=None):\n",
    "        X_s = X - self.shifts\n",
    "        if self.ARD:\n",
    "            if X2 is None:\n",
    "                return tdot(X_s*np.sqrt(self.variances))\n",
    "            else:\n",
    "                X2_s = X2 - self.shifts\n",
    "                rv = np.sqrt(self.variances)\n",
    "                return np.dot(X_s*rv, (X2_s*rv).T)\n",
    "        else:\n",
    "            if X2 is None:\n",
    "                X2_s = X_s\n",
    "            else:\n",
    "                X2_s = X2 - self.shifts\n",
    "            return self._dot_product(X_s, X2_s) * self.variances\n",
    "\n",
    "    @Cache_this(limit=3, ignore_args=(0,))\n",
    "    def _dot_product(self, X, X2=None):\n",
    "        if X2 is None:\n",
    "            return tdot(X)\n",
    "        else:\n",
    "            return np.dot(X, X2.T)\n",
    "\n",
    "    def Kdiag(self, X):\n",
    "        return np.sum(self.variances * np.square(X - self.shifts), -1)\n",
    "\n",
    "    def update_gradients_full(self, dL_dK, X, X2=None):\n",
    "        X_s = X - self.shifts\n",
    "        if X2 is None: \n",
    "            dL_dK = (dL_dK+dL_dK.T)/2\n",
    "            X2 = X\n",
    "            X2_s = X_s\n",
    "        else:\n",
    "            X2_s = X2 - self.shifts\n",
    "        \n",
    "        if self.ARD:\n",
    "            if X2 is None:\n",
    "                #self.variances.gradient = np.array([np.sum(dL_dK * tdot(X[:, i:i + 1])) for i in range(self.input_dim)])\n",
    "                self.variances.gradient = (dL_dK.dot(X_s)*X_s).sum(0) #np.einsum('ij,iq,jq->q', dL_dK, X, X)\n",
    "                self.shifts.gradient = (2*self.shifts - X - X2).sum(0)\n",
    "            else:\n",
    "                #product = X[:, None, :] * X2[None, :, :]\n",
    "                #self.variances.gradient = (dL_dK[:, :, None] * product).sum(0).sum(0)\n",
    "                self.variances.gradient = (dL_dK.dot(X2_s)*X_s).sum(0)  #np.einsum('ij,iq,jq->q', dL_dK, X, X2)\n",
    "        else:\n",
    "            self.variances.gradient = np.sum(self._dot_product(X_s, X2_s) * dL_dK)\n",
    "            self.shifts.gradient = np.sum((2*self.shifts - X - X2) * dL_dK)\n",
    "\n",
    "    def update_gradients_diag(self, dL_dKdiag, X):\n",
    "        X_s = X - self.shifts\n",
    "        tmp = dL_dKdiag[:, None] * X_s ** 2\n",
    "        if self.ARD:\n",
    "            self.variances.gradient = tmp.sum(0)\n",
    "        else:\n",
    "            self.variances.gradient = np.atleast_1d(tmp.sum())\n",
    "\n",
    "\n",
    "    def gradients_X(self, dL_dK, X, X2=None):\n",
    "        if X2 is None: \n",
    "            dL_dK = (dL_dK+dL_dK.T)/2\n",
    "        if X2 is None:\n",
    "            \n",
    "            return dL_dK.dot(X_s) * (2*self.variances) #np.einsum('jq,q,ij->iq', X, 2*self.variances, dL_dK)\n",
    "        else:\n",
    "            #return (((X2[None,:, :] * self.variances)) * dL_dK[:, :, None]).sum(1)\n",
    "            X2_s = X2 - self.shifts\n",
    "            return dL_dK.dot(X2_s) * self.variances #np.einsum('jq,q,ij->iq', X2, self.variances, dL_dK)\n",
    "\n",
    "    def gradients_XX(self, dL_dK, X, X2=None):\n",
    "        \"\"\"\n",
    "        Given the derivative of the objective K(dL_dK), compute the second derivative of K wrt X and X2:\n",
    "\n",
    "        returns the full covariance matrix [QxQ] of the input dimensionfor each pair or vectors, thus\n",
    "        the returned array is of shape [NxNxQxQ].\n",
    "\n",
    "        ..math:\n",
    "            \\frac{\\partial^2 K}{\\partial X2 ^2} = - \\frac{\\partial^2 K}{\\partial X\\partial X2}\n",
    "\n",
    "        ..returns:\n",
    "            dL2_dXdX2:  [NxMxQxQ] for X [NxQ] and X2[MxQ] (X2 is X if, X2 is None)\n",
    "                        Thus, we return the second derivative in X2.\n",
    "        \"\"\"\n",
    "        if X2 is None:\n",
    "            X2 = X\n",
    "        return np.zeros((X.shape[0], X2.shape[0], X.shape[1], X.shape[1]))\n",
    "        #if X2 is None: dL_dK = (dL_dK+dL_dK.T)/2\n",
    "        #if X2 is None:\n",
    "        #    return np.ones(np.repeat(X.shape, 2)) * (self.variances[None,:] + self.variances[:, None])[None, None, :, :]\n",
    "        #else:\n",
    "        #    return np.ones((X.shape[0], X2.shape[0], X.shape[1], X.shape[1])) * (self.variances[None,:] + self.variances[:, None])[None, None, :, :]\n",
    "\n",
    "\n",
    "    def gradients_X_diag(self, dL_dKdiag, X):\n",
    "        X_s = X - self.shifts\n",
    "        return 2.*self.variances*dL_dKdiag[:,None]*X_s\n",
    "\n",
    "    def gradients_XX_diag(self, dL_dKdiag, X):\n",
    "        return np.zeros((X.shape[0], X.shape[1], X.shape[1]))\n",
    "\n",
    "        #dims = X.shape\n",
    "        #if cov:\n",
    "        #    dims += (X.shape[1],)\n",
    "        #return 2*np.ones(dims)*self.variances\n",
    "\n",
    "    def input_sensitivity(self, summarize=True):\n",
    "        return np.ones(self.input_dim) * self.variances\n",
    "\n",
    "    #---------------------------------------#\n",
    "    #             PSI statistics            #\n",
    "    #---------------------------------------#\n",
    "\n",
    "    def psi0(self, Z, variational_posterior):\n",
    "        return self.psicomp.psicomputations(self, Z, variational_posterior)[0]\n",
    "\n",
    "    def psi1(self, Z, variational_posterior):\n",
    "        return self.psicomp.psicomputations(self, Z, variational_posterior)[1]\n",
    "\n",
    "    def psi2(self, Z, variational_posterior):\n",
    "        return self.psicomp.psicomputations(self, Z, variational_posterior)[2]\n",
    "\n",
    "    def psi2n(self, Z, variational_posterior):\n",
    "        return self.psicomp.psicomputations(self, Z, variational_posterior, return_psi2_n=True)[2]\n",
    "\n",
    "    def update_gradients_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n",
    "        dL_dvar = self.psicomp.psiDerivativecomputations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior)[0]\n",
    "        if self.ARD:\n",
    "            self.variances.gradient = dL_dvar\n",
    "        else:\n",
    "            self.variances.gradient = dL_dvar.sum()\n",
    "\n",
    "    def gradients_Z_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n",
    "        return self.psicomp.psiDerivativecomputations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior)[1]\n",
    "\n",
    "    def gradients_qX_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n",
    "        return self.psicomp.psiDerivativecomputations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior)[2:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Implementation\n",
    "Use GPML's `{@covSum, {@covConst, @covLinear}}` to test correctness.\n",
    "\n",
    "refer to https://github.com/jamesrobertlloyd/gpss-research/blob/master/source/gpml/cov/covLinear.m for `@covLinear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataplot': [<matplotlib.collections.PathCollection at 0x2513d326e80>],\n",
       " 'gpmean': [[<matplotlib.lines.Line2D at 0x2513d326d30>]],\n",
       " 'gpconfidence': [<matplotlib.collections.PolyCollection at 0x2513db993c8>]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\lousc\\Anaconda3\\envs\\py3-6\\lib\\site-packages\\matplotlib\\figure.py:2369: UserWarning:This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y = np.array([1, 2, 3, 2.1, 1.1]).reshape(-1, 1) \n",
    "\n",
    "\n",
    "ell_1 = 2.\n",
    "sf  = 1.5**2\n",
    "sn  = 1.2**2\n",
    "\n",
    "k = GPy.kern.Bias(1, variance=sn) + LinScaleShift(1, variances=sf, shifts=ell_1)\n",
    "\n",
    "gpml_result = np.array([\n",
    "    [3.69000000000000, 1.44000000000000, -0.810000000000000, -3.06000000000000, -5.31000000000000],\n",
    "    [1.44000000000000, 1.44000000000000, 1.44000000000000, 1.44000000000000, 1.44000000000000],\n",
    "    [-0.810000000000000, 1.44000000000000, 3.69000000000000, 5.94000000000000, 8.19000000000000],\n",
    "    [-3.06000000000000, 1.44000000000000, 5.94000000000000, 10.4400000000000, 14.9400000000000],\n",
    "    [-5.31000000000000, 1.44000000000000, 8.19000000000000, 14.9400000000000, 21.6900000000000]])\n",
    "\n",
    "# Test covariance matrix correctness\n",
    "cov_mat = k.K(X, X)\n",
    "assert np.allclose(cov_mat, gpml_result)\n",
    "\n",
    "m = GPy.models.GPRegression(X, y, kernel=k, noise_var=1.**2)\n",
    "m.optimize_restarts(verbose=False)\n",
    "m.plot()\n",
    "\n",
    "# TODO: Test negative log marginal likelihood correctness\n",
    "# gpml_nlml = ...\n",
    "# assert np.isclose(gpml_nlml, -m.log_likelihood(), rtol=1e-03)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
