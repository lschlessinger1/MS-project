{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Best Model Evidence Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "from src.autoks.postprocessing.summarize_group import _parse_experiment_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Default Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parsing params\n",
    "result_dir = Path(\"results\")\n",
    "\n",
    "# Visualization params\n",
    "save_fig = False\n",
    "output_path = 'best_model_evidence.pdf'\n",
    "fig_title = None\n",
    "x_label = 'evaluations'\n",
    "\n",
    "MAX_N_COLS = 3\n",
    "subplot_aspect_ratio = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Experiment Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format result directory\n",
    "(if a custom value is given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(result_dir, str):\n",
    "    result_dir = Path(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Result Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [result_dir / exp_group_dir for exp_group_dir in experiment_dir_names]\n",
    "print(f'Created {len(paths)} paths.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dicts_list = [_parse_experiment_group(p) for p in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dict of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "\n",
    "for exp_group_dict in exp_dicts_list:\n",
    "    for exp_dict in exp_group_dict:\n",
    "        # Get model selector.\n",
    "        model_selector = exp_dict['model_selector']\n",
    "        strategy_label = model_selector.__class__.__name__.split('ModelSelector')[0]\n",
    "    \n",
    "        # Get model search history.\n",
    "        history = exp_dict[\"history\"]\n",
    "        \n",
    "        # Get dataset.\n",
    "        datasets_module = importlib.import_module('src.datasets')\n",
    "        dataset_class_ = getattr(datasets_module, exp_dict['dataset_cls'])\n",
    "        dataset_args = exp_dict.get('dataset_args', {})\n",
    "        dataset = dataset_class_(**dataset_args)\n",
    "\n",
    "        best_scores = history.stat_book_collection.stat_books['evaluations'].running_max('score')\n",
    "\n",
    "        # Add to result dict.\n",
    "        ds_key = dataset.name.lower()\n",
    "        strat_key = strategy_label.lower()\n",
    "        if ds_key not in result_dict:\n",
    "            result_dict.update({ds_key: {strat_key: [best_scores]}})\n",
    "        else:\n",
    "            ds_values = result_dict[ds_key]\n",
    "            if strat_key not in ds_values:\n",
    "                result_dict[ds_key].update({strat_key: [best_scores]})\n",
    "            else:\n",
    "                result_dict[ds_key][strat_key].append(best_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize `result_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Created result dict.\\n')\n",
    "ds_keys = result_dict.keys()\n",
    "for ds_key, ds_val in result_dict.items():\n",
    "    n_strats = len(ds_val)\n",
    "    strat_label = 'strategy' if n_strats == 1 else 'strategies'\n",
    "    print(f'{ds_key} ({n_strats} {strat_label})')\n",
    "    \n",
    "    for strat_key, strat_val in ds_val.items():\n",
    "        n_runs = len(strat_val)\n",
    "        runs_label = 'run' if n_runs == 1 else 'runs'\n",
    "        print(f'   {strat_key} ({n_runs} {runs_label})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use result dict for visualization \n",
    "Example `best_scores_data`\n",
    "```\n",
    "{\n",
    "    'airline': {\n",
    "        'boems': [0.1, 0.2, 0.3, 0.4, 0.4],\n",
    "        'cks': [[0.1, 0.1, 0.2, 0.2, 0.25]]\n",
    "    },\n",
    "    'mauna': {\n",
    "        'cks': [[1, 2, 3, 4, 5], [1, 2, 3, 3, 5], [1, 1, 2, 4, 5]]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_scores_data = result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dataset labels, strategy labels, and data.\n",
    "best_score_keys = best_scores_data.keys()\n",
    "strategy_keys = tuple(best_scores_data[key].keys() for key in best_score_keys)\n",
    "\n",
    "dataset_labels = tuple(key.upper() for key in best_score_keys)\n",
    "strategy_labels = tuple(tuple(key.upper() for key in keys) for keys in strategy_keys)\n",
    "\n",
    "print(f\"Dataset labels:\\n   {dataset_labels}\")\n",
    "print()\n",
    "print(f\"Strategy labels:\\n   {strategy_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Force best score data to be 2D numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_key, dataset_values in best_scores_data.items():\n",
    "    for strategy_key, data in dataset_values.items():\n",
    "        new_data = np.array(data)\n",
    "\n",
    "        if new_data.ndim == 1:\n",
    "            new_data = new_data[:, None].T\n",
    "\n",
    "        assert new_data.ndim == 2\n",
    "        \n",
    "        best_scores_data[dataset_key][strategy_key] = new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Plotting Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_dataset_results(best_scores_list, ax, labels, title, legend=False):\n",
    "    title_kwargs = {\n",
    "        'size': 'large',\n",
    "        'weight': 'book',\n",
    "    }\n",
    "    \n",
    "    has_data = False\n",
    "    for best_scores, label in zip(best_scores_list, labels):\n",
    "        if best_scores.size > 0:\n",
    "            has_data = True\n",
    "            plot_mean_pm_std(best_scores, ax, label=label)\n",
    "    \n",
    "    ax.set_title(title, **title_kwargs)\n",
    "\n",
    "    if legend and has_data:\n",
    "        ax.legend(fontsize='large')\n",
    "\n",
    "def plot_mean_pm_std(data, ax=None, plot_confidence=True, **kwargs):\n",
    "    ax = ax or plt.gca()\n",
    "    x = np.arange(data.shape[1])\n",
    "    mu = np.mean(data, axis=0)\n",
    "    \n",
    "    if plot_confidence:\n",
    "        std = np.std(data, axis=0)\n",
    "        confidence = (mu - std, mu + std)\n",
    "        ax.fill_between(x, confidence[0], confidence[1], alpha=0.3)\n",
    "    \n",
    "    ax.margins(x=0)\n",
    "    return ax.plot(x, mu, lw=4, **kwargs)\n",
    "\n",
    "def hide_upper_ax_lines(ax):\n",
    "    # Hide the right and top spines\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    # Only show ticks on the left and bottom spines\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Global Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-paper')\n",
    "plt.rcParams['font.family'] = \"serif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer subplot ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# figure out plot alignment using MAX_N_COLS\n",
    "n_subplots = len(best_scores_data)\n",
    "\n",
    "# assume MAX_N_COLS = 3\n",
    "n_cols = min(MAX_N_COLS, n_subplots)\n",
    "n_rows = int(np.ceil(n_subplots / MAX_N_COLS))\n",
    "\n",
    "print(f'Going to create a plot with {n_subplots} subplots by allocating {n_rows} rows x {n_cols} columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer Figure Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "fig_subplot_width = 4.\n",
    "fig_subplot_height = fig_subplot_width / subplot_aspect_ratio\n",
    "\n",
    "fig_w = fig_subplot_width * n_cols\n",
    "fig_h = fig_subplot_height * n_rows\n",
    "\n",
    "figsize = (fig_w, fig_h)\n",
    "\n",
    "print(f\"Figure size = {figsize}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, Create Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axes =  plt.subplots(nrows=n_rows, ncols=n_cols, figsize=figsize)\n",
    "\n",
    "if n_rows == 1 and n_cols == 1:\n",
    "    axes = np.array([axes])\n",
    "\n",
    "i = 0\n",
    "for key, dataset_label, strategy_label in zip(best_score_keys, dataset_labels, strategy_labels):\n",
    "    row = i // n_rows#i // (n_subplots - 1)\n",
    "    col = i - row * n_rows#i % (n_subplots - 1)\n",
    "\n",
    "    best_scores_list = best_scores_data[key].values()\n",
    "\n",
    "    if n_rows == 1:\n",
    "        axis = axes[i]\n",
    "    else:\n",
    "        axis = axes[row, col]\n",
    "\n",
    "    plot_dataset_results(best_scores_list, axis, strategy_label, dataset_label, legend=True)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "# Format subplots.\n",
    "for ax in axes.reshape(-1): \n",
    "    hide_upper_ax_lines(ax)\n",
    "    ax.set_xlabel(x_label, size='large', weight= 'book')\n",
    "    ax.locator_params(nbins=5, axis='y')\n",
    "    ax.locator_params(nbins=4, axis='x')\n",
    "    \n",
    "    # Set the tick labels font.\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(12)\n",
    "        \n",
    "# Hide unused subplots.\n",
    "for i in range(n_subplots, int(n_rows * n_cols)):\n",
    "    row = i // n_rows#i // (n_subplots - 1)\n",
    "    col = i - row * n_rows#i % (n_subplots - 1)\n",
    "    \n",
    "    if n_rows == 1:\n",
    "        axis = axes[i]\n",
    "    else:\n",
    "        axis = axes[row, col]\n",
    "        \n",
    "    axis.axis('off')\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.4)\n",
    "\n",
    "if fig_title:\n",
    "    plt.suptitle(fig_title)\n",
    "\n",
    "if save_fig:\n",
    "    print(f'Saving figure to {output_path}')\n",
    "    plt.savefig(output_path)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (MS-project)",
   "language": "python",
   "name": "pycharm-fec2d9f9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}